---
title: "Unsupervised learning - clustering and dimension reduction"
author: "Changxuan Wu"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
```

## Homework

```{r,echo=FALSE,message=FALSE}
rm(list = ls(all.names = TRUE))
library(ggplot2)
library(tidyverse)
library(ggfortify)
library(fastICA)
data(iris)
```


0. Subset the Iris dataset to only include `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`. 

```{r}
iris_subset <- iris[c('Sepal.Length', 'Sepal.Width', 'Petal.Length','Petal.Width')]
```

1. Write out the Kmeans algorithm by hand, and run two iterations of it. 

Assume k = 2, We start with the first points in the dataset

```{r}
# find two point in the data that are most distant make them the initial centroids
d = as.matrix(dist(iris_subset))
cood = which(d == max(d), arr.ind = TRUE)
centroids = rbind(iris_subset[cood[1],], iris_subset[cood[2],])
print(centroids)
```


```{r}
# assign the point to cluster with closer centroid(first iteration)
cluster_one = c()
cluster_two = c()
for (i in 1:150){
  dist_one = dist(rbind(centroids[1,],iris_subset[i,]), method = "euclidean")[1]
  dist_two = dist(rbind(centroids[2,],iris_subset[i,]), method = "euclidean")[1]
  if (dist_one < dist_two){
    cluster_one = rbind(cluster_one, iris_subset[i,])
  }else {
    cluster_two = rbind(cluster_two, iris_subset[i,])
  }
}
nrow(cluster_one)
nrow(cluster_two)
```


```{r}
# recalculate the centroids 
centroids = rbind(t(as.matrix(colMeans(cluster_one))),t(as.matrix(colMeans(cluster_two))))
print(centroids)
# and again assign the point to cluster with closer centroid(second iteration)
cluster_one = c()
cluster_two = c()
for (i in 1:150){
  dist_one = dist(rbind(centroids[1,],iris_subset[i,]), method = "euclidean")[1]
  dist_two = dist(rbind(centroids[2,],iris_subset[i,]), method = "euclidean")[1]
  if (dist_one < dist_two){
    cluster_one = rbind(cluster_one, iris_subset[i,])
  }else {
    cluster_two = rbind(cluster_two, iris_subset[i,])
  }
}
nrow(cluster_one)
nrow(cluster_two)
```


2. Run PCA on the Iris dataset. Plot a scatter plot of PC1 vs PC2 and include the percent variance those PCs describe. 

```{r}
pca_iris = prcomp(iris_subset)
autoplot(pca_iris)
```

3. Run ICA on the Iris dataset. Plot the independent components as a heatmap.

```{r, message= FALSE, results='hide'}
a <- fastICA(iris_subset, 4, alg.typ = "parallel", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)
heatmap(a$S)
```

4. Use Kmeans to cluster the Iris data. 
  
  * Use the silhouette function in the cluster package to find the optimal number of clusters for kmeans for the iris dataset. Then cluster using kmeans clustering. Does the data cluster by species? 
  
  * Using this clustering, color the PCA plot according to the clusters.
  
```{r}
library(cluster)
sil_avg_width = c()
for (k in 2:20){
  avg_width = 0
  for (i in 1:10){
      a <- kmeans(iris_subset, k, iter.max = 10000, 
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen"), trace=FALSE)
      b <- silhouette(a$cluster, dist(iris_subset))
      avg_width <- avg_width +summary(b)$avg.width
  }
  sil_avg_width <- c(sil_avg_width, avg_width/10)
}
plot(sil_avg_width,xlab = "Cluster number", ylab = "Average sil width")
```

2 clusters seems to be the best 

```{r}
a <- kmeans(iris_subset, 2, iter.max = 10000, 
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen"), trace=FALSE)
a$cluster
```

Almost all setosas(~50) are in one cluster, the other cluster (~100) include both versicolor and virginica 

```{r}
pca_iris = prcomp(iris_subset)

autoplot(prcomp(iris_subset), data = iris, colour = a$cluster)
```

5. Use hierarchical clustering to cluster the Iris data.

  * Try two different linkage types, and two different distance metrics. 

  * For one linkage type and one distance metric, try two different cut points. 

  * Using this clustering, color the PCA plot according to the clusters. (6  plots in total)

```{r, warning= FALSE}
hierarchical_dist <- dist(iris_subset, method = "euclidean")
tree1 <- hclust(hierarchical_dist, method="average")
plot(tree1)
tree2 <- hclust(hierarchical_dist, method="complete")
plot(tree2)
hierarchical_dist <- dist(iris_subset, method = "manhattan")
tree3 <- hclust(hierarchical_dist, method="average")
plot(tree3)
tree4 <- hclust(hierarchical_dist, method="complete")
plot(tree4)
rect.hclust(tree4, k = 2, h = NULL)
cutree(tree4, 2)

plot(tree4)
rect.hclust(tree4, k = 3, h = NULL)
cutree(tree4, 3)

plot(tree4)
rect.hclust(tree4, k = 4, h = NULL)
cutree(tree4, 4)

autoplot(prcomp(iris_subset), colour = cutree(tree1, 2))
autoplot(prcomp(iris_subset), colour = cutree(tree2, 2))
autoplot(prcomp(iris_subset), colour = cutree(tree3, 2))
autoplot(prcomp(iris_subset), colour = cutree(tree4, 2))
autoplot(prcomp(iris_subset), colour = cutree(tree4, 3))
autoplot(prcomp(iris_subset), colour = cutree(tree4, 4))


``` 

# Optional material
On PCA:

Eigen Vectors and Eigen Values http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/
Linear Algebra by Prof. Gilbert Strang https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/
http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf
https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues

On ICA: 

Independent Component Analysis: Algorithms and Applications https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf
Tutorial on ICA taken from http://rstudio-pubs-static.s3.amazonaws.com/93614_be30df613b2a4707b3e5a1a62f631d19.html



